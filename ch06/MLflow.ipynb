{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlflow-energyforecast\n",
    "\n",
    "This is a showcase for ML Flow capabilities, based on the article\n",
    "http://the-odd-dataguy.com/be-more-efficient-to-produce-ml-models-with-mlflow\n",
    "and a github https://github.com/jeanmidevacc/mlflow-energyforecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in ./.local/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.6.0.tar.gz (15.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.9 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic\n",
      "  Downloading alembic-1.4.0.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 44.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: click>=7.0 in ./.local/lib/python3.6/site-packages (from mlflow) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in ./.local/lib/python3.6/site-packages (from mlflow) (1.1.1)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.9.1.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/lib/python3/dist-packages (from mlflow) (1.11.0)\n",
      "Collecting Flask\n",
      "  Downloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 4.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in ./.local/lib/python3.6/site-packages (from mlflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.11.2)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.0.8-py3-none-any.whl (450 kB)\n",
      "\u001b[K     |████████████████████████████████| 450 kB 69.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (5.3)\n",
      "Collecting querystring_parser\n",
      "  Downloading querystring_parser-1.2.4.tar.gz (5.5 kB)\n",
      "Collecting simplejson\n",
      "  Downloading simplejson-3.17.0.tar.gz (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: docker>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n",
      "Collecting sqlparse\n",
      "  Downloading sqlparse-0.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.3.13.tar.gz (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 51.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gorilla\n",
      "  Downloading gorilla-0.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.12.2.tar.gz (18 kB)\n",
      "Collecting gunicorn\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.1.1.tar.gz (468 kB)\n",
      "\u001b[K     |████████████████████████████████| 468 kB 45.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in ./.local/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: configparser>=0.3.5 in ./.local/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow) (4.0.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.17.3->mlflow) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.6/site-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.11.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (0.16.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->mlflow) (45.1.0)\n",
      "Collecting gitdb2>=3\n",
      "  Downloading gitdb2-3.0.2-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 451 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=4.0.0->mlflow) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: prometheus_client in /usr/local/lib/python3.6/dist-packages (from prometheus-flask-exporter->mlflow) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->mlflow) (1.1.1)\n",
      "Collecting smmap2>=2.0.0\n",
      "  Downloading smmap2-2.0.5-py2.py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: mlflow, alembic, databricks-cli, querystring-parser, simplejson, sqlalchemy, prometheus-flask-exporter, Mako\n",
      "  Building wheel for mlflow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mlflow: filename=mlflow-1.6.0-py3-none-any.whl size=16101858 sha256=ee94455432f3e22aef44b2a7e88e865cf72f1f6bce1f010b8e2457d1043ba265\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/52/17/45/1370dd1b969ca154f4cd7db298a76d17b2a817426c8c9bf1e5\n",
      "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alembic: filename=alembic-1.4.0-py2.py3-none-any.whl size=159981 sha256=fe67d2411cb2dde912dc58a8f1a0075a3355660bb216817584edd2eaeac717ad\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/54/88/28/d771a55dfb3c62af8b3358c60f8034edcb5c9a57d44a9024cf\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.9.1-py3-none-any.whl size=85835 sha256=5e6d95132657cae9382d07433bebe247e91ed6f366193d92e657cdd2d3653d34\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3d/27/8f/ccd6bc76a2efcabd1ed7f8656ac432f7fb165d62f2c7df9cf8\n",
      "  Building wheel for querystring-parser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-py3-none-any.whl size=7967 sha256=bacc2e0ad6beed80bc2d8f4a8d1df5a2f5642ef281e853588e58375bb82102d4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/87/da/0d/a9e8051c7c19128c271107462cd444b37afc22aaed15061ed5\n",
      "  Building wheel for simplejson (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for simplejson: filename=simplejson-3.17.0-cp36-cp36m-linux_x86_64.whl size=111948 sha256=76509d7da3c70ee5cbad9e8453ffd031ab0ec1d3905c5d4c8bd18fe1c9f9ba8d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/c2/95/a2/477b46cfe980061fddc4cb76ed4657eceec35d2b2090355e41\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlalchemy: filename=SQLAlchemy-1.3.13-cp36-cp36m-linux_x86_64.whl size=1220669 sha256=51c4c080c3d9fe4f5df9211e019b4ab93462d27d082ce57836104318f61bcb6b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/28/3e/f9/8eca04781258bb6956ffba37e4e6e6951e5b3a16d4494b91cb\n",
      "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.12.2-py3-none-any.whl size=19288 sha256=9ea67300d4fba6ca7676acbfeebd7fcff43cf9013d378b7ae9a421fa89c9e0f3\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6e/cc/3691602eeca883fa3db046605c675989117c9dbfa13ed46f34\n",
      "  Building wheel for Mako (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Mako: filename=Mako-1.1.1-py3-none-any.whl size=76163 sha256=c36c837fad25c221708b35cc5f2decd74c584a60f92dfd4fe373ac316385551e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/43/b1/7c/f14ef20f4683e5087ae684c6447194e09695315f20b9c45575\n",
      "Successfully built mlflow alembic databricks-cli querystring-parser simplejson sqlalchemy prometheus-flask-exporter Mako\n",
      "Installing collected packages: sqlalchemy, Mako, python-editor, alembic, databricks-cli, itsdangerous, Flask, smmap2, gitdb2, gitpython, querystring-parser, simplejson, sqlparse, gorilla, prometheus-flask-exporter, gunicorn, mlflow\n",
      "\u001b[33m  WARNING: The script mako-render is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script alembic is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts databricks and dbfs are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script flask is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sqlformat is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gunicorn is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mlflow is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Flask-1.1.1 Mako-1.1.1 alembic-1.4.0 databricks-cli-0.9.1 gitdb2-3.0.2 gitpython-3.0.8 gorilla-0.3.0 gunicorn-20.0.4 itsdangerous-1.1.0 mlflow-1.6.0 prometheus-flask-exporter-0.12.2 python-editor-1.0.4 querystring-parser-1.2.4 simplejson-3.17.0 smmap2-2.0.5 sqlalchemy-1.3.13 sqlparse-0.3.0\n",
      "Collecting joblib\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib\n",
      "Successfully installed joblib-0.14.1\n",
      "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.18.1)\n",
      "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in ./.local/lib/python3.6/site-packages (from scikit-learn) (0.14.1)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-0.22.1\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.12.1-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.4-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.16.0,>=1.15.1\n",
      "  Downloading botocore-1.15.1-py2.py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 81.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 8.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.1->boto3) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 45.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3<1.26,>=1.20; python_version != \"3.4\" in ./.local/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.1->boto3) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.1->boto3) (1.11.0)\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.12.1 botocore-1.15.1 docutils-0.15.2 jmespath-0.9.4 s3transfer-0.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --upgrade --user\n",
    "!pip install mlflow --upgrade --user\n",
    "!pip install joblib --upgrade --user\n",
    "!pip install numpy --upgrade --user \n",
    "!pip install scipy --upgrade --user \n",
    "!pip install scikit-learn --upgrade --user\n",
    "!pip install boto3 --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from  mlflow.tracking import MlflowClient\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category = FutureWarning)\n",
    "simplefilter(action='ignore', category = ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Minio access\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://minio-service.kubeflow.svc.cluster.local:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data \n",
    "df_nationalconsumption_electricity_daily = pd.read_csv(\"https://raw.githubusercontent.com/jeanmidevacc/mlflow-energyforecast/master/data/rtu_data.csv\")\n",
    "df_nationalconsumption_electricity_daily.set_index([\"day\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set :  1081\n",
      "Size of the testing set :  233\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training set and the testing set\n",
    "df_trainvalidate_energyconsumption = df_nationalconsumption_electricity_daily[df_nationalconsumption_electricity_daily[\"datastatus\"] == \"Définitif\"]\n",
    "del df_trainvalidate_energyconsumption[\"datastatus\"]\n",
    "\n",
    "df_test_energyconsumption = df_nationalconsumption_electricity_daily[df_nationalconsumption_electricity_daily[\"datastatus\"] == \"Consolidé\"]\n",
    "del df_test_energyconsumption[\"datastatus\"]\n",
    "\n",
    "print(\"Size of the training set : \",len(df_trainvalidate_energyconsumption))\n",
    "print(\"Size of the testing set : \",len(df_test_energyconsumption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to predict :  dailyconsumption\n",
      "Inputs for the prediction :  ['weekday', 'week', 'month', 'year', 'avg_min_temperature', 'avg_max_temperature', 'avg_mean_temperature', 'wavg_min_temperature', 'wavg_max_temperature', 'wavg_mean_temperature', 'is_holiday']\n"
     ]
    }
   ],
   "source": [
    "# Define the inputs and the output\n",
    "output = \"dailyconsumption\"\n",
    "allinputs = list(df_trainvalidate_energyconsumption.columns)\n",
    "allinputs.remove(output)\n",
    "\n",
    "print(\"Output to predict : \", output)\n",
    "print(\"Inputs for the prediction : \", allinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build different set of featurws for the model\n",
    "possible_inputs = {\n",
    "    \"all\" : allinputs,\n",
    "    \"only_allday_inputs\" : [\"weekday\", \"month\", \"is_holiday\", \"week\"],\n",
    "    \"only_allweatheravg_inputs\" : [\"avg_min_temperature\", \"avg_max_temperature\", \"avg_mean_temperature\",\"wavg_min_temperature\", \"wavg_max_temperature\", \"wavg_mean_temperature\"],\n",
    "    \"only_meanweather_inputs_avg\" : [\"avg_mean_temperature\"],\n",
    "    \"only_meanweather_inputs_wavg\" : [\"wavg_mean_temperature\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the output of the model\n",
    "array_output_train = np.array(df_trainvalidate_energyconsumption[output])\n",
    "array_output_test = np.array(df_test_energyconsumption[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'electricityconsumption-forecast' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "# connect to remote server\n",
    "remote_server_uri = \"http://mlflowserver.kubeflow.svc.cluster.local:5000\"\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "# Launch the experiment on mlflow\n",
    "experiment_name = \"electricityconsumption-forecast\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function that will do the computation of the different metrics of accuracy (RMSE,MAE,R2)\n",
    "def evaluation_model(y_test, y_pred):\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"rmse\" : rmse,\n",
    "        \"r2\" : r2,\n",
    "        \"mae\" : mae,\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def train_knnmodel(parameters, inputs, tags, log = False):\n",
    "    with mlflow.start_run(nested = True):\n",
    "        \n",
    "        # Prepare the data\n",
    "        array_inputs_train = np.array(df_trainvalidate_energyconsumption[inputs])\n",
    "        array_inputs_test = np.array(df_test_energyconsumption[inputs])\n",
    "        \n",
    "        \n",
    "        # Build the model\n",
    "        tic = time.time()\n",
    "        model = KNeighborsRegressor(parameters[\"nbr_neighbors\"], weights = parameters[\"weight_method\"])\n",
    "        model.fit(array_inputs_train, array_output_train)\n",
    "        duration_training = time.time() - tic\n",
    "\n",
    "        # Make the prediction\n",
    "        tic1 = time.time()\n",
    "        prediction = model.predict(array_inputs_test)\n",
    "        duration_prediction = time.time() - tic1\n",
    "\n",
    "        # Evaluate the model prediction\n",
    "        metrics = evaluation_model(array_output_test, prediction)\n",
    "\n",
    "        # Log in the console\n",
    "        if log:\n",
    "            print(f\"KNN regressor:\")\n",
    "            print(parameters)\n",
    "            print(metrics)\n",
    "\n",
    "        # Log in mlflow (parameter)\n",
    "        mlflow.log_params(parameters)\n",
    "\n",
    "        # Log in mlflow (metrics)\n",
    "        metrics[\"duration_training\"] = duration_training\n",
    "        metrics[\"duration_prediction\"] = duration_prediction\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log in mlflow (model)\n",
    "        mlflow.sklearn.log_model(model, f\"model\")\n",
    "                \n",
    "        # Tag the model\n",
    "        mlflow.set_tags(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the different combinations\n",
    "configurations = []\n",
    "for nbr_neighbors in [1,2,5,10]:\n",
    "    for weight_method in ['uniform','distance']:\n",
    "        for field in possible_inputs:\n",
    "            parameters = {\n",
    "                \"nbr_neighbors\" : nbr_neighbors,\n",
    "                \"weight_method\" : weight_method\n",
    "            }\n",
    "\n",
    "            tags = {\n",
    "                \"model\" : \"knn\",\n",
    "                \"inputs\" : field\n",
    "            }\n",
    "            \n",
    "            configurations.append([parameters, tags])\n",
    "\n",
    "            train_knnmodel(parameters, possible_inputs[field], tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def train_mlpmodel(parameters, inputs, tags, log = False):\n",
    "    with mlflow.start_run(nested = True):\n",
    "        \n",
    "        # Prepare the data\n",
    "        array_inputs_train = np.array(df_trainvalidate_energyconsumption[inputs])\n",
    "        array_inputs_test = np.array(df_test_energyconsumption[inputs])\n",
    "        \n",
    "        # Build the model\n",
    "        tic = time.time()\n",
    "\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes = parameters[\"hidden_layers\"],\n",
    "            activation = parameters[\"activation\"],\n",
    "            solver = parameters[\"solver\"],\n",
    "            max_iter = parameters[\"nbr_iteration\"],\n",
    "            random_state = 0)\n",
    "        \n",
    "        model.fit(array_inputs_train, array_output_train)\n",
    "        duration_training = time.time() - tic\n",
    "\n",
    "        # Make the prediction\n",
    "        tic1 = time.time()\n",
    "        prediction = model.predict(array_inputs_test)\n",
    "        duration_prediction = time.time() - tic1\n",
    "\n",
    "        # Evaluate the model prediction\n",
    "        metrics = evaluation_model(array_output_test, prediction)\n",
    "\n",
    "        # Log in the console\n",
    "        if log:\n",
    "            print(f\"Random forest regressor:\")\n",
    "            print(parameters)\n",
    "            print(metrics)\n",
    "    \n",
    "        # Log in mlflow (parameter)\n",
    "        mlflow.log_params(parameters)\n",
    "\n",
    "        # Log in mlflow (metrics)\n",
    "        metrics[\"duration_training\"] = duration_training\n",
    "        metrics[\"duration_prediction\"] = duration_prediction\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log in mlflow (model)\n",
    "        mlflow.sklearn.log_model(model, f\"model\")\n",
    "        \n",
    "        # Tag the model\n",
    "        mlflow.set_tags(tags)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hiddenlayers in [4,8,16]:\n",
    "    for activation in [\"identity\",\"logistic\",]:\n",
    "        for solver in [\"lbfgs\"]:\n",
    "            for nbriteration in [10,100,1000]:\n",
    "                for field in possible_inputs:\n",
    "                    parameters = {\n",
    "                        \"hidden_layers\" : hiddenlayers,\n",
    "                        \"activation\" : activation,\n",
    "                        \"solver\" : solver,\n",
    "                        \"nbr_iteration\" : nbriteration\n",
    "                    }\n",
    "\n",
    "                    tags = {\n",
    "                        \"model\" : \"mlp\",\n",
    "                        \"inputs\" : field\n",
    "                    }\n",
    "\n",
    "                    train_mlpmodel(parameters, possible_inputs[field], tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a handmade model (scipy approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTG:\n",
    "    def __init__(self, thresholds_x0, thresholds_a, thresholds_b):\n",
    "        self.thresholds_x0 = thresholds_x0\n",
    "        self.thresholds_a = thresholds_a\n",
    "        self.thresholds_b = thresholds_b\n",
    "        \n",
    "    def get_ptgmodel(self, x, a, b, x0):\n",
    "        return np.piecewise(x, [x < x0, x >= x0], [lambda x: a*x + b , lambda x : a*x0 + b])\n",
    "        \n",
    "    def fit(self, dfx, y):\n",
    "        x = np.array(dfx)\n",
    "        \n",
    "        # Define the bounds\n",
    "        bounds_min = [thresholds_a[0], thresholds_b[0], thresholds_x0[0]]\n",
    "        bounds_max = [thresholds_a[1], thresholds_b[1], thresholds_x0[1]]\n",
    "        bounds = (bounds_min, bounds_max)\n",
    "\n",
    "        # Fit a model\n",
    "        popt, pcov = scipy.optimize.curve_fit(self.get_ptgmodel, x, y, bounds = bounds)\n",
    "\n",
    "        # Get the parameter of the model\n",
    "        a = popt[0]\n",
    "        b = popt[1]\n",
    "        x0 = popt[2]\n",
    "        \n",
    "        self.coefficients = [a, b, x0]\n",
    "        \n",
    "    def predict(self,dfx):\n",
    "        x = np.array(dfx)\n",
    "        predictions = []\n",
    "        for elt in x:\n",
    "            forecast = self.get_ptgmodel(elt, self.coefficients[0], self.coefficients[1], self.coefficients[2])\n",
    "            predictions.append(forecast)\n",
    "        return np.array(predictions)\n",
    "        \n",
    "def train_ptgmodel(parameters, inputs, tags, log = False):\n",
    "    with mlflow.start_run(nested = True):\n",
    "        \n",
    "        # Prepare the data\n",
    "        df_inputs_train = df_trainvalidate_energyconsumption[inputs[0]]\n",
    "        df_inputs_test = df_test_energyconsumption[inputs[0]]\n",
    "        \n",
    "        \n",
    "        # Build the model\n",
    "        tic = time.time()\n",
    "        \n",
    "        model = PTG(parameters[\"thresholds_x0\"], parameters[\"thresholds_a\"], parameters[\"thresholds_b\"])\n",
    "        \n",
    "        model.fit(df_inputs_train, array_output_train)\n",
    "        duration_training = time.time() - tic\n",
    "\n",
    "        # Make the prediction\n",
    "        tic1 = time.time()\n",
    "        prediction = model.predict(df_inputs_test)\n",
    "        duration_prediction = time.time() - tic1\n",
    "\n",
    "        # Evaluate the model prediction\n",
    "        metrics = evaluation_model(array_output_test, prediction)\n",
    "\n",
    "        # Log in the console\n",
    "        if log:\n",
    "            print(f\"PTG:\")\n",
    "            print(parameters)\n",
    "            print(metrics)\n",
    "    \n",
    "        # Log in mlflow (parameter)\n",
    "        mlflow.log_params(parameters)  \n",
    "\n",
    "        # Log in mlflow (metrics)\n",
    "        metrics[\"duration_training\"] = duration_training\n",
    "        metrics[\"duration_prediction\"] = duration_prediction\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log in mlflow (model)\n",
    "        mlflow.sklearn.log_model(model, f\"model\")\n",
    "        \n",
    "        # Tag the model\n",
    "        mlflow.set_tags(tags)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the model\n",
    "thresholds_x0 = [0, 20]\n",
    "thresholds_a = [-200000, -50000]\n",
    "thresholds_b = [1000000, 3000000]\n",
    "\n",
    "parameters = {\n",
    "    \"thresholds_x0\" : thresholds_x0,\n",
    "    \"thresholds_a\" : thresholds_a,\n",
    "    \"thresholds_b\" : thresholds_b\n",
    "}\n",
    "\n",
    "for field in [\"only_meanweather_inputs_avg\", \"only_meanweather_inputs_wavg\"]:\n",
    "    \n",
    "    tags = {\n",
    "        \"model\" : \"ptg\",\n",
    "        \"inputs\" : field\n",
    "    }\n",
    "    \n",
    "    train_ptgmodel(parameters, possible_inputs[field], tags, log = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate mlflow results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs done :  132\n"
     ]
    }
   ],
   "source": [
    "# Select the run of the experiment\n",
    "df_runs = mlflow.search_runs(experiment_ids=\"0\")\n",
    "print(\"Number of runs done : \", len(df_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.duration_prediction</th>\n",
       "      <th>metrics.rmse</th>\n",
       "      <th>metrics.r2</th>\n",
       "      <th>metrics.mae</th>\n",
       "      <th>...</th>\n",
       "      <th>params.solver</th>\n",
       "      <th>params.hidden_layers</th>\n",
       "      <th>params.activation</th>\n",
       "      <th>params.weight_method</th>\n",
       "      <th>params.nbr_neighbors</th>\n",
       "      <th>tags.model</th>\n",
       "      <th>tags.inputs</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2f39bfaa9f274f3bab7b936b420f1d46</td>\n",
       "      <td>0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>s3://mlflow/mlflow/artifacts/0/2f39bfaa9f274f3...</td>\n",
       "      <td>2020-02-18 00:54:53.284000+00:00</td>\n",
       "      <td>2020-02-18 00:54:53.704000+00:00</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>134649.399348</td>\n",
       "      <td>0.935956</td>\n",
       "      <td>104040.339809</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>distance</td>\n",
       "      <td>5</td>\n",
       "      <td>knn</td>\n",
       "      <td>all</td>\n",
       "      <td>/usr/local/lib/python3.6/dist-packages/ipykern...</td>\n",
       "      <td>jovyan</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>44f15c7e8ca649c5a74196595179f38b</td>\n",
       "      <td>0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>s3://mlflow/mlflow/artifacts/0/44f15c7e8ca649c...</td>\n",
       "      <td>2020-02-18 00:54:57.359000+00:00</td>\n",
       "      <td>2020-02-18 00:54:57.804000+00:00</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>135534.759873</td>\n",
       "      <td>0.935111</td>\n",
       "      <td>105833.358681</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>distance</td>\n",
       "      <td>10</td>\n",
       "      <td>knn</td>\n",
       "      <td>all</td>\n",
       "      <td>/usr/local/lib/python3.6/dist-packages/ipykern...</td>\n",
       "      <td>jovyan</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>70e6f927c62f41a78cae9c735450884c</td>\n",
       "      <td>0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>s3://mlflow/mlflow/artifacts/0/70e6f927c62f41a...</td>\n",
       "      <td>2020-02-18 00:54:51.313000+00:00</td>\n",
       "      <td>2020-02-18 00:54:51.737000+00:00</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>136207.422483</td>\n",
       "      <td>0.934465</td>\n",
       "      <td>105793.727897</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5</td>\n",
       "      <td>knn</td>\n",
       "      <td>all</td>\n",
       "      <td>/usr/local/lib/python3.6/dist-packages/ipykern...</td>\n",
       "      <td>jovyan</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2263a05359904da5a75df58a18c5a60f</td>\n",
       "      <td>0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>s3://mlflow/mlflow/artifacts/0/2263a05359904da...</td>\n",
       "      <td>2020-02-18 00:54:55.461000+00:00</td>\n",
       "      <td>2020-02-18 00:54:55.808000+00:00</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>138279.158616</td>\n",
       "      <td>0.932457</td>\n",
       "      <td>108427.970386</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>uniform</td>\n",
       "      <td>10</td>\n",
       "      <td>knn</td>\n",
       "      <td>all</td>\n",
       "      <td>/usr/local/lib/python3.6/dist-packages/ipykern...</td>\n",
       "      <td>jovyan</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>312fcbab17bf452b867f57f7f9f23121</td>\n",
       "      <td>0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>s3://mlflow/mlflow/artifacts/0/312fcbab17bf452...</td>\n",
       "      <td>2020-02-18 00:54:49.147000+00:00</td>\n",
       "      <td>2020-02-18 00:54:49.530000+00:00</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>148886.582823</td>\n",
       "      <td>0.921697</td>\n",
       "      <td>114048.572635</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>knn</td>\n",
       "      <td>all</td>\n",
       "      <td>/usr/local/lib/python3.6/dist-packages/ipykern...</td>\n",
       "      <td>jovyan</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               run_id experiment_id    status  \\\n",
       "106  2f39bfaa9f274f3bab7b936b420f1d46             0  FINISHED   \n",
       "96   44f15c7e8ca649c5a74196595179f38b             0  FINISHED   \n",
       "111  70e6f927c62f41a78cae9c735450884c             0  FINISHED   \n",
       "101  2263a05359904da5a75df58a18c5a60f             0  FINISHED   \n",
       "116  312fcbab17bf452b867f57f7f9f23121             0  FINISHED   \n",
       "\n",
       "                                          artifact_uri  \\\n",
       "106  s3://mlflow/mlflow/artifacts/0/2f39bfaa9f274f3...   \n",
       "96   s3://mlflow/mlflow/artifacts/0/44f15c7e8ca649c...   \n",
       "111  s3://mlflow/mlflow/artifacts/0/70e6f927c62f41a...   \n",
       "101  s3://mlflow/mlflow/artifacts/0/2263a05359904da...   \n",
       "116  s3://mlflow/mlflow/artifacts/0/312fcbab17bf452...   \n",
       "\n",
       "                          start_time                         end_time  \\\n",
       "106 2020-02-18 00:54:53.284000+00:00 2020-02-18 00:54:53.704000+00:00   \n",
       "96  2020-02-18 00:54:57.359000+00:00 2020-02-18 00:54:57.804000+00:00   \n",
       "111 2020-02-18 00:54:51.313000+00:00 2020-02-18 00:54:51.737000+00:00   \n",
       "101 2020-02-18 00:54:55.461000+00:00 2020-02-18 00:54:55.808000+00:00   \n",
       "116 2020-02-18 00:54:49.147000+00:00 2020-02-18 00:54:49.530000+00:00   \n",
       "\n",
       "     metrics.duration_prediction   metrics.rmse  metrics.r2    metrics.mae  \\\n",
       "106                     0.002390  134649.399348    0.935956  104040.339809   \n",
       "96                      0.003756  135534.759873    0.935111  105833.358681   \n",
       "111                     0.002399  136207.422483    0.934465  105793.727897   \n",
       "101                     0.002650  138279.158616    0.932457  108427.970386   \n",
       "116                     0.002118  148886.582823    0.921697  114048.572635   \n",
       "\n",
       "     ...  params.solver params.hidden_layers params.activation  \\\n",
       "106  ...           None                 None              None   \n",
       "96   ...           None                 None              None   \n",
       "111  ...           None                 None              None   \n",
       "101  ...           None                 None              None   \n",
       "116  ...           None                 None              None   \n",
       "\n",
       "    params.weight_method params.nbr_neighbors tags.model tags.inputs  \\\n",
       "106             distance                    5        knn         all   \n",
       "96              distance                   10        knn         all   \n",
       "111              uniform                    5        knn         all   \n",
       "101              uniform                   10        knn         all   \n",
       "116             distance                    2        knn         all   \n",
       "\n",
       "                               tags.mlflow.source.name tags.mlflow.user  \\\n",
       "106  /usr/local/lib/python3.6/dist-packages/ipykern...           jovyan   \n",
       "96   /usr/local/lib/python3.6/dist-packages/ipykern...           jovyan   \n",
       "111  /usr/local/lib/python3.6/dist-packages/ipykern...           jovyan   \n",
       "101  /usr/local/lib/python3.6/dist-packages/ipykern...           jovyan   \n",
       "116  /usr/local/lib/python3.6/dist-packages/ipykern...           jovyan   \n",
       "\n",
       "    tags.mlflow.source.type  \n",
       "106                   LOCAL  \n",
       "96                    LOCAL  \n",
       "111                   LOCAL  \n",
       "101                   LOCAL  \n",
       "116                   LOCAL  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick sorting to get the best models based on the RMSE metric\n",
    "df_runs.sort_values([\"metrics.rmse\"], ascending = True, inplace = True)\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2f39bfaa9f274f3bab7b936b420f1d46'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best one\n",
    "runid_selected = df_runs.head(1)[\"run_id\"].values[0]\n",
    "runid_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
